{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIDI Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment command below to kill current job:\n",
    "#!neuro kill $(hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import subprocess\n",
    "import torch\n",
    "sys.path.append('../midi-generator')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "from model.dataset import MidiDataset\n",
    "\n",
    "from utils.load_model import load_model\n",
    "from utils.generate_midi import generate_midi\n",
    "from utils.seed import set_seed\n",
    "from utils.write_notes import write_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `*.mid` file can be thought of as a sequence where notes and chords follow each other with specified time offsets between them. So, following this model a next note can be predicted with a `seq2seq` model. In this work, a simple `GRU`-based model is used.\n",
    "\n",
    "Note that the number of available notes and chord in vocabulary is not specified and depends on a dataset which a model was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To listen to MIDI files from Jupyter notebook, let's define help function which transforms `*.mid` file to `*.wav` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid2wav(mid_path, wav_path):\n",
    "    subprocess.check_output(['timidity', mid_path, '-OwS', '-o', wav_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is loading the model from the checkpoint. To make experiments reproducible let's also specify random seed.\n",
    "\n",
    "You can also try to use the model, which was trained with label smoothing (see `../results/smoothing.ch`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "set_seed(seed)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "model, vocab = load_model(checkpoint_path='../results/test.ch', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also specify additional help function to avoid code duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_result(file_preffix, vocab, note_seq, offset_seq=None):\n",
    "    note_seq = vocab.decode(note_seq)\n",
    "    notes = MidiDataset.decode_notes(note_seq, offset_seq=offset_seq)\n",
    "\n",
    "    mid_path = file_preffix + '.mid'\n",
    "    wav_path = file_preffix + '.wav'\n",
    "\n",
    "    write_notes(mid_path, notes)\n",
    "    mid2wav(mid_path, wav_path)\n",
    "    \n",
    "    return wav_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDI file generation\n",
    "\n",
    "Let's generate a new file. Note that the parameter `seq_len` specifies the length of the output sequence of notes. \n",
    "\n",
    "Function `generate_midi` return sequence of generated notes and offsets between them.\n",
    "\n",
    "## Nucleus (`top-p`) Sampling\n",
    "\n",
    "Sample from the most probable tokens, which sum of probabilities gives `top-p`.  If `top-p == 0` the most probable token is sampled.\n",
    "\n",
    "## Temperature\n",
    "\n",
    "As `temperature` → 0 this approaches greedy decoding, while `temperature` → ∞ asymptotically approaches uniform sampling from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_seq, offset_seq = generate_midi(model, vocab, seq_len=128, top_p=0, temperature=1, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's listen to result midi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi with constant offsets\n",
    "ipd.Audio(dump_result('../results/output_without_offsets', vocab, note_seq, offset_seq=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi with generated offsets\n",
    "ipd.Audio(dump_result('../results/output_with_offsets.mid', vocab, note_seq, offset_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result with constant offsets sounds better, doesn't it? :)\n",
    "\n",
    "Be free to try different generation parameters (`top-p` and `temperature`) to understand their impact on the resulting sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also train your own model with different specs (e.g. different hidden size) or use label smoothing during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue existing file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue sampled notes\n",
    "For beginning, let's continue sound that consists of sampled from `vocab` notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 4321\n",
    "set_seed(seed)\n",
    "\n",
    "history_notes = random.choices(range(len(vocab)), k=20)\n",
    "history_offsets = len(history_notes) * [0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(dump_result('../results/random_history', vocab, history_notes, history_offsets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sounds a little bit chaotic. Let's try to continue this with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [*zip(history_notes, history_offsets)]\n",
    "note_seq, offset_seq = generate_midi(model, vocab, seq_len=128, top_p=0, temperature=1, device=device, \n",
    "                                     history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi with constant offsets\n",
    "ipd.Audio(dump_result('../results/random_without_offsets', vocab, note_seq, offset_seq=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the sampled part ends, the generated melody starts to sound better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue existed melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_notest = MidiDataset.load_raw_notes('../data/mining.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_note_seq, org_offset_seq = MidiDataset.encode_notes(raw_notest)\n",
    "org_note_seq = vocab.encode(org_note_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's listen to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(dump_result('../results/original_sound', vocab, org_note_seq, org_offset_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and take 20 first elements from the sequence as out history sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_notes = org_note_seq[:20]\n",
    "history_offsets = org_offset_seq[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [*zip(history_notes, history_offsets)]\n",
    "note_seq, offset_seq = generate_midi(model, vocab, seq_len=128, top_p=0, temperature=1, device=device, \n",
    "                                     history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result melody without generated offsets\n",
    "ipd.Audio(dump_result('../results/continue_rand_without_offsets', vocab, note_seq, offset_seq=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result melody with generated offsets\n",
    "ipd.Audio(dump_result('../results/continue_rand_with_offsets', vocab, note_seq, offset_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to overfit your model on one melody to get better results. Otherwise, you can use already pretrained model (`../results/onemelody.ch`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model overfitted on one melody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same thing which we did before. Let's continue melody, but this time do it with the model, \n",
    "which was overfitted with this melody."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "set_seed(seed)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model, vocab = load_model(checkpoint_path='../results/onemelody.ch', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_notest = MidiDataset.load_raw_notes('../data/Final_Fantasy_Matouyas_Cave_Piano.mid')\n",
    "org_note_seq, org_offset_seq = MidiDataset.encode_notes(raw_notest)\n",
    "org_note_seq = vocab.encode(org_note_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's listen to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(dump_result('../results/onemelody_original_sound', vocab, org_note_seq, org_offset_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = 60\n",
    "history_notes = org_note_seq[:end]\n",
    "history_offsets = org_offset_seq[:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to history part of loaded melody."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(dump_result('../results/onemelody_history', vocab, history_notes, history_offsets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to continue the original melody with our model. But firstly, you can listen to the original tail part of the melody do refresh it in the memory and have reference to compare with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_notes = org_note_seq[end:]\n",
    "tail_offsets = org_offset_seq[end:]\n",
    "ipd.Audio(dump_result('../results/onemelody_tail', vocab, tail_notes, tail_offsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [*zip(history_notes, history_offsets)]\n",
    "note_seq, offset_seq = generate_midi(model, vocab, seq_len=128, top_p=0, temperature=1, device=device, \n",
    "                                     history=history)\n",
    "\n",
    "# delete history part\n",
    "note_seq = note_seq[end:]\n",
    "offset_seq = offset_seq[end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result melody without generated offsets\n",
    "ipd.Audio(dump_result('../results/continue_onemelody_without_offsets', vocab, note_seq, offset_seq=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result melody with generated offsets\n",
    "ipd.Audio(dump_result('../results/continue_onemelody_with_offsets', vocab, note_seq, offset_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can hear, this time, the model generated better offsets and the result melody does not sound so chaostic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
