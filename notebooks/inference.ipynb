{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import torch\n",
    "sys.path.append('../midi-generator')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "from model.dataset import MidiDataset\n",
    "\n",
    "from utils.load_model import load_model\n",
    "from utils.generate_midi import generate_midi\n",
    "from utils.seed import set_seed\n",
    "from utils.write_notes import write_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To listen to midi files from Jupyter notebook, let's define help function which transforms `*.mid` file to `*.wav` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid2wav(mid_path, wav_path):\n",
    "    subprocess.check_output(['timidity', mid_path, '-OwS', '-o', wav_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is loading the model from the checkpoint. To make experiments reproducible let's also specify random seed.\n",
    "\n",
    "You can also try to use the model, which was trained with label smoothing (see `../results/smoothing.ch`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "set_seed(seed)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model, vocab = load_model(checkpoint_path='../results/test.ch', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDI file generation\n",
    "\n",
    "Let's generate a new file. Notice, that the parameter `seq_len` specifies the length of the output sequence of notes. \n",
    "\n",
    "Function `generate_midi` return sequence of generated notes and offsets between them.\n",
    "\n",
    "## Nucleus (`top-p`) Sampling\n",
    "\n",
    "Sample from the most probable tokens, which sum of probabilities gives `top-p`.  If `top-p == 0` the most probable token is sampled.\n",
    "\n",
    "## Temperature\n",
    "\n",
    "As `temperature` → 0 this approaches greedy decoding, while `temperature` → ∞ asymptotically approaches uniform sampling from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ac7c6570f345b59fbc83321f049ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1024), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "note_seq, offset_seq = generate_midi(model, vocab, seq_len=1024, top_p=0, temperature=1, device=device)\n",
    "note_seq = vocab.decode(note_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's listen to result midi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi with constant offsets\n",
    "notes = MidiDataset.decode_notes(note_seq, offset_seq=None)\n",
    "\n",
    "mid_path = '../results/output_without_offsets.mid'\n",
    "wav_path = '../results/output_without_offsets.wav'\n",
    "\n",
    "write_notes(mid_path, notes)\n",
    "mid2wav(mid_path, wav_path)\n",
    "\n",
    "ipd.Audio(wav_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi with generated offsets\n",
    "notes = MidiDataset.decode_notes(note_seq, offset_seq=offset_seq)\n",
    "\n",
    "mid_path = '../results/output_with_offsets.mid'\n",
    "wav_path = '../results/output_with_offsets.wav'\n",
    "\n",
    "write_notes(mid_path, notes)\n",
    "mid2wav(mid_path, wav_path)\n",
    "\n",
    "ipd.Audio(wav_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result with constant offsets sounds better, doesn't it? :)\n",
    "\n",
    "Be free to try different generation parameters (`top-p` and `temperature`) to understand their impact on the resulting sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also train your own model with different specs (e.g. different hidden size) or use label smoothing during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
